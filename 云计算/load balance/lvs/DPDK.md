DPDK说白了就是一种用户态的协议栈，dpdk 绕过了 Linux 内核协议栈对数据包的处理过程，
在用户空间实现了一套数据平面来进行数据包的收发与处理。在内核看来，
dpdk 就是一个普通的用户态进程，它的编译、连接和加载方式和普通程序没有什么两样。

## 基于DPDK的4层负载均衡
不仅可以快速的横向扩展，还可以最大限度的提升负载均衡单个NIC的处理转发速度，来实现L4的负载均衡。借助DPDK的优势，如便利的多核编程框架、
巨页内存管理、无锁队列、无中断poll-mode 网卡驱动、CPU亲和性等等来实现快速的
网卡收发及处理报文，后续考虑TCP/IP 用户态协议实现和优化，进而实现L7负载均衡。

DPDK和LVS结合，LVS是实现负载均衡，而DPDK主要是为了提高转发的性能（速度）。
所以说，DPDK不仅可以用于负载均衡，还可以用于其他内核系统提升性能。

###爱奇艺实现了一个基于DPDK的LVS 4层均衡DPVS（百度也是基于DPDK的4层负载均衡）
传统模式下，应用程序如nginx等运行在用户态（User Space），网卡（NIC）属于硬件设备，是归属于内核管理，网卡驱动运行在内核态中（Kernel Space）。那么当应用程序需要和网卡交换数据的时候，需要进行一轮用户态到内核态之间的切换，再经过内核中的TCP/IP协议栈，才能和网卡驱动通信，而网卡驱动和网卡之间的通信是通过硬件中断的方式来实现的，也就是传统模式下存在着用户态/内核态切换和两个非常耗时耗资源的操作。

DPDK的处理方式非常简单粗暴，首先将网卡驱动从内核态移动到用户态运行，这样应用程序和PMD网卡驱动之间进行数据交换就不需要进行内核态/用户态的切换，避免了上下文切换，且由于都是在用户态，不需要将数据拷贝到内核态，也实现了零拷贝zero copy，同时还绕过了内核中的TCP/IP网络栈，极大地缩短APP到Driver之间的传输时间。

当然这种绕过系统内核的做法最直接的代价就是驱动程序必须一直通过轮询poll的操作来保证能够及时接收到网卡的信息和数据，导致对应的cpu核心会一直处于100%的占用状态。同时这个时候网卡已经不再归于系统内核管理，常规的ip和ifconfig等命令已经没办法查看网卡的详细信息，需要使用dpip工具来对网卡进行管理。

这里就是dpvs宣称的内核旁路（kernel bypass）、轮询（polling）、零拷贝（zero copy）等特性的实现原理。
